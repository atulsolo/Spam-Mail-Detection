{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c25a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, explore and plot data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# suppress display of warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79cff43",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "For Example: Mangoes ---> Mango\n",
    "\n",
    "             Boys ---> Boy\n",
    "             \n",
    "             going ---> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e0427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sshiv\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ea60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plays  :  play\n",
      "playing  :  play\n",
      "played  :  play\n",
      "player  :  player\n",
      "pharmacies  :  pharmaci\n",
      "badly  :  badli\n",
      "improvement  :  improv\n",
      "hospitals  :  hospit\n"
     ]
    }
   ],
   "source": [
    "#importing nltk's porter stemmer \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "#from nltk.tokenize import word_tokenize \n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "words = ['plays', 'playing', 'played', 'player', 'pharmacies', 'badly', 'improvement', 'hospitals']\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce6fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plays  :  play\n",
      "playing  :  play\n",
      "played  :  play\n",
      "player  :  player\n",
      "pharmacies  :  pharmaci\n",
      "badly  :  bad\n",
      "improvement  :  improv\n",
      "hospitals  :  hospit\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#from nltk.tokenize import word_tokenize \n",
    "\n",
    "sb = SnowballStemmer(language = 'english')\n",
    "\n",
    "\n",
    "words = ['plays', 'playing', 'played', 'player', 'pharmacies', 'badly', 'improvement', 'hospitals']\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" : \", sb.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5978f9",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be43e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plays  :  play\n",
      "playing  :  playing\n",
      "played  :  played\n",
      "player  :  player\n",
      "pharmacies  :  pharmacy\n",
      "badly  :  badly\n",
      "improvement  :  improvement\n",
      "hospitals  :  hospital\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "#from nltk.tokenize import word_tokenize \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "words = ['plays', 'playing', 'played', 'player', 'pharmacies', 'badly', 'improvement', 'hospitals']\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" : \", lemma.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e71352",
   "metadata": {},
   "source": [
    "Stemming is faster to implement and quite straightforward. There might be some inaccuracies, they may be irrelevant to particular tasks and operations.\n",
    "\n",
    "Lemmatization, on the other hand, provides better results by analyzing the POS of the words and thus displaying real words. Lemmatization is harder to implement and a bit slower when compared to Stemming.\n",
    "\n",
    "In short, Lemmatization is the best choice when you are looking for qualitative results. In the modern day, Lemmatization algorithms do not affect the performance. But if you want to optimize speed then Stemming algorithms are the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac82d30",
   "metadata": {},
   "source": [
    "### *Bag-of-words using Count Vectorization*\n",
    "\n",
    "*The bag-of-words model converts text into fixed-length vectors by counting how many times each word appears. Let us illustrate this with an example. Consider that we have the following sentences:*\n",
    "\n",
    "- Text processing is necessary.\n",
    "- Text processing is necessary and important.\n",
    "- Text processing is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85191eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Text processing is necessary.', 'Text processing is necessary and important.', 'Text processing is easy.']\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179d2eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f83700",
   "metadata": {},
   "source": [
    "#### Limitations of Bag-of-Words:\n",
    "\n",
    "- If we deploy bag-of-words to generate vectors for large documents, the vectors would be of large sizes and would also have too many null values leading to the creation of sparse vectors.\n",
    "- Bag-of-words does not bring in any information on the meaning of the text. For example, if we consider these two sentences – “Text processing is easy but tedious.” and “Text processing is tedious but easy.” – a bag-of-words model would create the same vectors for both of them, even though they have different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecf4c2",
   "metadata": {},
   "source": [
    "### Term Frequency Inverse Document Frequency (TF-IDF) :\n",
    "\n",
    "*TFIDF works by proportionally increasing the number of times a word appears in the document but is counterbalanced by the number of documents in which it is present. Hence, words like ‘this’, ’are’ etc., that are commonly present in all the documents are not given a very high rank. However, a word that is present too many times in a few of the documents will be given a higher rank as it might be indicative of the context of the document.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80932a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.46333427 0.59662724 0.46333427\n",
      "  0.46333427]\n",
      " [0.52523431 0.         0.52523431 0.31021184 0.39945423 0.31021184\n",
      "  0.31021184]\n",
      " [0.         0.69903033 0.         0.41285857 0.         0.41285857\n",
      "  0.41285857]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61285a8a",
   "metadata": {},
   "source": [
    "#### Important parameters to know – Sklearn’s CountVectorizer & TFIDF vectorization:\n",
    "\n",
    "- *max_features: This parameter enables using only the ‘n’ most frequent words as features instead of all the words. An integer can be passed for this parameter.*\n",
    "\n",
    "\n",
    "- *stop_words: You could remove the extremely common words like ‘this’, ’is’, ’are’ etc by using this parameter as the common words add little value to the model. We can set the parameter to ‘english’ to use a built-in list. We can also set this parameter to a custom list.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- *ngram_range: An n-gram is a string of words in a row. For example, in the sentence – “Text processing is easy.”, 2-grams could be ‘Text processing’, ‘processing is’ or ‘is easy’. We can set the ngram_range to be (x,y) where x is the minimum and y is the maximum size of the n-grams we want to include in the features. The default ngram_range is (1,1).*\n",
    "\n",
    "\n",
    "- *min_df, max_df: These refer to the minimum and maximum document frequency that a word/n-gram should have to be used as a feature. The frequency here refers to the proportion of documents. Both the parameters have to be set in the range of [0,1].*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a172a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
